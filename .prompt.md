## task context

I am participating in a global hackathon

**Main Prizes**

*   **1st Place:**
    *   $15k Gemini Credits
    *   $15k OpenAI Credits
    *   $5k JigsawStack Credits
    *   $2.5k Cursor Credits
    *   $1K Groq Credits
    *   $300 Supabase Credits per member
    *   Manus Pro (3 Months)
    *   ElevenLabs Pro (3 Months)

**Side Track Prizes**

*   **ElevenLabs:** Best use of ElevenLabs, an AI voice generator.
    *   Each team member receives 6 months of the Scale tier.
*   **Gemini API:** Top 3 teams using the Gemini API.
    *   $10K credits for each of the Top 3 teams.
*   **Groq:** Best use of Groq, an AI chip company that builds hardware for AI inference.
    *   $1,000 in API Credits, a shoutout on Groq socials, and exclusive swag.
*   **Mem0:** Best Use of Mem0, a universal, self-improving memory layer for LLM applications.
    *   6 months of Mem0 Pro under their Startup Program and a shoutout on Mem0’s social channels.

### **Lumina: AI Discussion Facilitator Tech Spec**

**Objective:** A voice-activated AI agent, Lumina, that analyzes small-group classroom discussions in real-time. It provides on-demand, supportive verbal interventions to keep students on topic and engaged, while providing teachers with a high-level dashboard of group performance.

**Tech Stack & Architecture:**
*   **Full-Stack Framework:** Next.js
*   **Language:** TypeScript
*   **UI:** React (via Next.js), Tailwind CSS
*   **Backend Logic:** Next.js API Routes will handle all server-side operations, including communication with external AI services.
*   **AI - Vercel AI SDK:** https://sdk.vercel.ai/
*   **AI - Analysis & Transcription:** Groq (handles real-time transcription, speaker diarization, and analysis of conversational health metrics).
*   **AI - Voice Synthesis:** ElevenLabs
*   **AI - Memory:** (optional) Mem0 to give AI a persistent memory of the conversation.
*   **Hardware:** Single laptop with a microphone per group.

**Environment variables**
- ELEVENLABS_API_KEY
- OPENAI_API_KEY (we will use openai for now)

**Core Features:**

**1. Student Dashboard:**
*   **Voice Onboarding:** Each of the 5 students in a group registers their voice for speaker identification.
*   **Main Interface:** Displays the teacher-assigned discussion prompt (e.g., "Should school students be forced to wear school uniforms"), a countdown timer, and the Lumina chatbot interface.

**2. Teacher Dashboard:**
*   Provides a high-level overview of all groups in the classroom.
*   Displays real-time status and alerts for groups that have received "Severe" intervention warnings, allowing the teacher to provide targeted support.

**Server-Side Logic & AI Integration (via Next.js API Routes):**
*   **Monitoring:** The Next.js backend continuously receives and processes the audio stream, sending it to Groq for analysis against conversational health triggers.
*   **Discussion Mode:** Teachers can tag prompts as "Breadth" or "Depth."
    *   **Breadth:** For topics requiring exploration of multiple viewpoints (e.g., "advantages & disadvantages of universal healthcare").
    *   **Depth:** For topics requiring focused, detailed analysis of a single concept (e.g., "how to write a PEEL paragraph").
*   **Intervention Triggers:**
    1.  **Silence / Low Participation:** Detects extended periods of no speech.
    2.  **Off Topic:** Identifies significant deviation from the core prompt.
    3.  **Imbalanced Discussion (Breadth Mode):** If students neglect key facets of a "Breadth" topic, Lumina prompts them to explore new perspectives.
*   **Two-Tier Intervention Flow:**
    1.  **Mild Intervention:**
        *   **Trigger:** A short period of silence or minor topic drift is detected.
        *   **Action:** The client-side UI displays a non-intrusive "Lumina wants to speak" notification. Students can click a button to hear Lumina's suggestion (e.g., a related question or a gentle nudge back to the topic). If ignored, the request times out.
    2.  **Severe Intervention:**
        *   **Trigger:** A prolonged period of silence or major topic drift is detected.
        *   **Action:** The API route generates a direct, helpful verbal intervention (e.g., "I've noticed we've been quiet for a bit. How about we consider this question...?") and plays it through the speakers via ElevenLabs.
        *   **Teacher Alert:** An alert is simultaneously sent to the Teacher Dashboard.

Refer to example.ts for styling and the layout:
1. 1st page registers the voice of each student (input student count)
2. 2nd page will list the discussion topic
3. 3rd page will be the actual main page for students with the entire interface

## [optional] tone context

## background data, documents, & images

file structure:
```
.
├── app
│   ├── favicon.ico
│   ├── globals.css
│   ├── layout.tsx
│   └── page.tsx
├── public
│   ├── file.svg
│   ├── globe.svg
│   ├── next.svg
│   ├── vercel.svg
│   └── window.svg
├── bun.lock
├── next.config.ts
├── package.json
├── postcss.config.mjs
├── README.md
└── tsconfig.json
```

Files you must read:
- `package.json` for dependencies
- `example.ts` for styling and the layout

## detailed task description & rules

### **Lumina: Updated Project Sub-tasks**

#### **Phase 1: UI Development & Styling**

*   **Task 1: Configure Tailwind CSS**
    *   **Goal:** Finalize the project's styling foundation.
    *   **Instructions:**
        *   Create and configure your `tailwind.config.ts` file to define your theme, colors, and content paths.
        *   Ensure your `postcss.config.js` is set up correctly.
        *   Import the core Tailwind CSS directives (`@tailwind base;`, `@tailwind components;`, `@tailwind utilities;`) into your main global stylesheet (e.g., `app/globals.css`).

*   **Task 2: Build Student Dashboard UI**
    *   **Goal:** Create the static React components for the student-facing dashboard.
    *   **Instructions:**
        *   In your `/app` directory, create a new page for the student view.
        *   Using React and Tailwind CSS, build the UI with placeholders for:
            *   The discussion prompt.
            *   A countdown timer.
            *   A simple chat interface for Lumina.
            *   A button with the text "Lumina wants to speak," which should be hidden by default.

*   **Task 3: Build Teacher Dashboard UI**
    *   **Goal:** Create the static React components for the teacher's overview dashboard.
    *   **Instructions:**
        *   Create a new page for the teacher view.
        *   Design a grid or list layout to display cards for multiple student groups.
        *   Each card should have UI elements for the group's name, a status indicator, and an area for alerts.

#### **Phase 2: Backend API & Core Logic**

*   **Task 4: Implement Real-time Audio Capture & Streaming**
    *   **Goal:** Capture microphone audio on the client-side and stream it to a new backend API route.
    *   **Instructions:**
        *   On the Student Dashboard page, use the browser's `MediaRecorder` API to capture audio.
        *   Create a Next.js API route (e.g., `/api/listen`).
        *   Implement logic to stream the captured audio chunks from the client to this API route.

*   **Task 5: Create the Core Analysis API Route (OpenAI/Groq)**
    *   **Goal:** Set up the Vercel AI SDK in your API route to transcribe and analyze the incoming audio stream.
    *   **Instructions:**
        *   In the `/api/listen` route, receive the audio stream.
        *   Use the Vercel AI SDK to forward this stream to an AI provider for transcription (start with OpenAI, as per your `package.json`).
        *   The route should process the real-time transcription results.

*   **Task 6: Develop Intervention Trigger Logic**
    *   **Goal:** Analyze the transcription to identify moments for intervention.
    *   **Instructions:**
        *   In the same API route, implement the server-side logic for the triggers:
            1.  **Silence:** Start a timer that resets every time a new piece of transcript is received.
            2.  **Off-Topic & Imbalance:** Periodically, send the recent conversation transcript to the OpenAI API (using the `generateText` function from the AI SDK) with a prompt asking it to check for topic deviation or discussion imbalance.
        *   This logic should determine whether to trigger a "Mild" or "Severe" intervention.

#### **Phase 3: AI Services & Frontend Integration**

*   **Task 7: Integrate ElevenLabs for Voice Synthesis**
    *   **Goal:** Create an API route that converts Lumina's intervention text into speech.
    *   **Instructions:**
        *   Create a new Next.js API route (`/api/speak`).
        *   This route will accept text as input, call the ElevenLabs API to generate audio, and stream the audio back to the client.

*   **Task 8: Implement the Two-Tier Intervention Flow**
    *   **Goal:** Connect the backend triggers to the frontend UI and audio playback.
    *   **Instructions:**
        *   **Mild Intervention:** When the backend detects a mild trigger, send a signal to the client to show the "Lumina wants to speak" button.
        *   **Severe Intervention:** When a severe trigger is detected, the backend should generate the intervention text, call the `/api/speak` route, and stream the resulting audio directly to the client for immediate playback.

*   **Task 9: Connect Teacher Dashboard to Backend**
    *   **Goal:** Feed real-time alerts from the backend to the Teacher Dashboard.
    *   **Instructions:**
        *   When a "Severe Intervention" occurs, the backend should push an alert to the Teacher Dashboard using a real-time communication method like Server-Sent Events.

## examples

## [optional] conversation history

## immediate task description or request

NOTE:
- use `bun` not `npm`

## [optional] thinking step by step / take a deep breath

## output formatting

## [optional] prefilled response

---

references:
- [Use prompt files in VS Code](https://code.visualstudio.com/docs/copilot/customization/prompt-files)
- [Prompting 101](https://youtube.com/watch?v=ysPbXH0LpIE)
