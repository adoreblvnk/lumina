## task context

I am participating in a global hackathon

**Main Prizes**

*   **1st Place:**
    *   $15k Gemini Credits
    *   $15k OpenAI Credits
    *   $5k JigsawStack Credits
    *   $2.5k Cursor Credits
    *   $1K Groq Credits
    *   $300 Supabase Credits per member
    *   Manus Pro (3 Months)
    *   ElevenLabs Pro (3 Months)

**Side Track Prizes**

*   **ElevenLabs:** Best use of ElevenLabs, an AI voice generator.
    *   Each team member receives 6 months of the Scale tier.
*   **Gemini API:** Top 3 teams using the Gemini API.
    *   $10K credits for each of the Top 3 teams.
*   **Groq:** Best use of Groq, an AI chip company that builds hardware for AI inference.
    *   $1,000 in API Credits, a shoutout on Groq socials, and exclusive swag.
*   **Mem0:** Best Use of Mem0, a universal, self-improving memory layer for LLM applications.
    *   6 months of Mem0 Pro under their Startup Program and a shoutout on Mem0â€™s social channels.

### **Lumina: AI Discussion Facilitator Tech Spec**

**Objective:** A voice-activated AI agent, Lumina, that analyzes small-group classroom discussions in real-time. It provides on-demand, supportive verbal interventions to keep students on topic and engaged, while providing teachers with a high-level dashboard of group performance.

**Tech Stack & Architecture:**
*   **Full-Stack Framework:** Next.js
*   **Language:** TypeScript
*   **UI:** React (via Next.js), Tailwind CSS
*   **Backend Logic:** Next.js API Routes will handle all server-side operations, including communication with external AI services.
*   **AI - Vercel AI SDK:** https://sdk.vercel.ai/
*   **AI - Analysis & Transcription:** Groq (handles real-time transcription, speaker diarization, and analysis of conversational health metrics).
*   **AI - Voice Synthesis:** ElevenLabs
*   **AI - Memory:** (optional) Mem0 to give AI a persistent memory of the conversation.
*   **Hardware:** Single laptop with a microphone per group.

**Environment variables**
- ELEVENLABS_API_KEY
- OPENAI_API_KEY (we will use openai for now)

**Core Features:**

**1. Student Dashboard:**
*   **Voice Onboarding:** Each of the 5 students in a group registers their voice for speaker identification.
*   **Main Interface:** Displays the teacher-assigned discussion prompt (e.g., "Should school students be forced to wear school uniforms"), a countdown timer, and the Lumina chatbot interface.

**2. Teacher Dashboard:**
*   Provides a high-level overview of all groups in the classroom.
*   Displays real-time status and alerts for groups that have received "Severe" intervention warnings, allowing the teacher to provide targeted support.

**Server-Side Logic & AI Integration (via Next.js API Routes):**
*   **Monitoring:** The Next.js backend continuously receives and processes the audio stream, sending it to Groq for analysis against conversational health triggers.
*   **Discussion Mode:** Teachers can tag prompts as "Breadth" or "Depth."
    *   **Breadth:** For topics requiring exploration of multiple viewpoints (e.g., "advantages & disadvantages of universal healthcare").
    *   **Depth:** For topics requiring focused, detailed analysis of a single concept (e.g., "how to write a PEEL paragraph").
*   **Intervention Triggers:**
    1.  **Silence / Low Participation:** Detects extended periods of no speech.
    2.  **Off Topic:** Identifies significant deviation from the core prompt.
    3.  **Imbalanced Discussion (Breadth Mode):** If students neglect key facets of a "Breadth" topic, Lumina prompts them to explore new perspectives.
*   **Two-Tier Intervention Flow:**
    1.  **Mild Intervention:**
        *   **Trigger:** A short period of silence or minor topic drift is detected.
        *   **Action:** The client-side UI displays a non-intrusive "Lumina wants to speak" notification. Students can click a button to hear Lumina's suggestion (e.g., a related question or a gentle nudge back to the topic). If ignored, the request times out.
    2.  **Severe Intervention:**
        *   **Trigger:** A prolonged period of silence or major topic drift is detected.
        *   **Action:** The API route generates a direct, helpful verbal intervention (e.g., "I've noticed we've been quiet for a bit. How about we consider this question...?") and plays it through the speakers via ElevenLabs.
        *   **Teacher Alert:** An alert is simultaneously sent to the Teacher Dashboard.

## [optional] tone context

## background data, documents, & images

file structure:
```
.
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”œâ”€â”€ generate-speech
â”‚   â”‚   â”‚   â””â”€â”€ route.ts
â”‚   â”‚   â”œâ”€â”€ signed-url
â”‚   â”‚   â”‚   â””â”€â”€ route.ts
â”‚   â”‚   â””â”€â”€ transcribe-name
â”‚   â”‚       â””â”€â”€ route.ts
â”‚   â”œâ”€â”€ hooks
â”‚   â”‚   â””â”€â”€ useAgentConversation.ts
â”‚   â”œâ”€â”€ teacher
â”‚   â”‚   â””â”€â”€ dashboard
â”‚   â”‚       â””â”€â”€ page.tsx
â”‚   â”œâ”€â”€ types
â”‚   â”‚   â””â”€â”€ websocket.ts
â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”œâ”€â”€ globals.css
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â””â”€â”€ page.tsx
â”œâ”€â”€ components
â”‚   â”œâ”€â”€ ui
â”‚   â”‚   â”œâ”€â”€ button.tsx
â”‚   â”‚   â”œâ”€â”€ card.tsx
â”‚   â”‚   â”œâ”€â”€ input.tsx
â”‚   â”‚   â””â”€â”€ orb.tsx
â”‚   â”œâ”€â”€ background-wave.tsx
â”‚   â”œâ”€â”€ DiscussionView.tsx
â”‚   â”œâ”€â”€ logos.tsx
â”‚   â””â”€â”€ Lumina.tsx
â”œâ”€â”€ lib
â”‚   â”œâ”€â”€ stream.ts
â”‚   â””â”€â”€ utils.ts
â”œâ”€â”€ public
â”‚   â””â”€â”€ wave-loop.mp4
â”œâ”€â”€ bun.lock
â”œâ”€â”€ components.json
â”œâ”€â”€ example.ts
â”œâ”€â”€ next.config.ts
â”œâ”€â”€ package.json
â”œâ”€â”€ postcss.config.mjs
â”œâ”€â”€ README.md
â”œâ”€â”€ tailwind.config.ts
â”œâ”€â”€ tsconfig.json
â””â”€â”€ websocket-server.js
```

How to use ai-sdk, an example of using ai-sdk is this:
Agent.ts
```ts
import { anthropic } from "@ai-sdk/anthropic";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";
import { Experimental_Agent as Agent, stepCountIs } from "ai";
import { createCodebaseTools } from "./tools/codebase-tools";
import type { AuthProfile } from "./types";
import { Sandbox } from "e2b";
import { EvalEngine } from "../EvalEngine";

export async function createTestGenAgent(
  sandbox: Sandbox,
  authProfile: AuthProfile,
  sandboxUrl: string | undefined = undefined
) {
  const tools = await createCodebaseTools(sandbox, sandboxUrl, authProfile);

  // Create base agent
  const baseAgent = new Agent({
    model: anthropic("claude-sonnet-4-5"),
    tools: {
      readFile: tools.readFileTool,
      globFiles: tools.globTool,
      searchCodebase: tools.grepTool,
      readTestFile: tools.readTestFile,
      writeTestFile: tools.writeTestFile,
      runTest: tools.runTest,
      checkServerStatus: tools.checkServerStatus,
      browser_navigate: tools.browser_navigate,
      browser_snapshot: tools.browser_snapshot,
      browser_click: tools.browser_click,
      browser_type: tools.browser_type,
      browser_verify_element_visible: tools.browser_verify_element_visible,
    },
    stopWhen: stepCountIs(100), // Increased limit for comprehensive code analysis, testing iteration, and test generation
  });

  // Wrap with evaluation capabilities
  const evalEngine = EvalEngine.create();
  const sheetId = "1kcLNHycMBUpkwlsaLHVeu0IY7t5U2EPtYuYjw05fDUU"; // Same sheet as email agent for now
  return evalEngine.wrap(baseAgent, "testgen-agent", sheetId);
}

```

AgentPrompt.ts
```ts
export function getTestgenAgentPrompt(
  issueTitle: string,
  issueBody: string | null,
  sandboxUrl: string,
  issueNumber: number
): string {
  return "You are a QA Engineer creating Playwright tests. Your ONLY goal: Write tests that verify the feature described in this issue works correctly.\n\n" +

    "ISSUE: " + issueTitle + "\n" +
    "DETAILS: " + (issueBody ?? "") + "\n" +
    "APP URL: " + sandboxUrl + "\n\n" +

    "=== YOUR SINGLE OBJECTIVE ===" +
    "\nðŸŽ¯ Write 1-3 tests that prove this specific feature works (or doesn't work)" +
    "\nðŸŽ¯ Nothing more, nothing less" +
    "\nðŸŽ¯ Don't test the entire app - just answer the issue" +

    // other code here
}

```

TestGenerator.ts
```ts
import { checkIssueIsAlreadyInDB } from "./utils/checkIssueIsAlreadyInDB";
import { SandboxEngine } from "../Sandboxes";
import { createTestGenAgent } from "./Agent";
import { FilestoreInterface } from "../Filestore";
import { getTestgenAgentPrompt } from "./AgentPrompt";
import type { 
  AuthProfile, 
  TestGenerationResult, 
  TestGenerationErrorType,
  TestGenerationSuccessDetails,
  TestGenerationFailureDetails
} from "./types";
import auth from "../../routes/auth";

export class TestGenerator {
  static async generateTest(params: GenerateTestParams): Promise<TestGenerationResult> {
        // other code here
        try {
          agentResult = await testGenAgent.generate({
            prompt: getTestgenAgentPrompt(
              issueTitle,
              issueBody,
              sandboxUrl,
              issueNumber
            ),
          });

// code here
```

here is how to use groq in ai-sdk:
```
You can import the default provider instance groq from @ai-sdk/groq:

import { groq } from '@ai-sdk/groq';

If you need a customized setup, you can import createGroq from @ai-sdk/groq and create a provider instance with your settings:

import { createGroq } from '@ai-sdk/groq';

const groq = createGroq({
  // custom settings
});

You can use the following optional settings to customize the Groq provider instance:

    baseURL string

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is https://api.groq.com/openai/v1.

    apiKey string

    API key that is being sent using the Authorization header. It defaults to the GROQ_API_KEY environment variable.

    headers Record<string,string>

    Custom headers to include in the requests.

    fetch (input: RequestInfo, init?: RequestInit) => Promise<Response>

    Custom fetch

implementation. Defaults to the global fetch function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.
```

here is how to use elevenlabs with aisdk:
```txt
You can create models that call the ElevenLabs transcription API

using the .transcription() factory method.

The first argument is the model id e.g. scribe_v1.

const model = elevenlabs.transcription('scribe_v1');

You can also pass additional provider-specific options using the providerOptions argument. For example, supplying the input language in ISO-639-1 (e.g. en) format can sometimes improve transcription performance if known beforehand.

import { experimental_transcribe as transcribe } from 'ai';
import { elevenlabs } from '@ai-sdk/elevenlabs';

const result = await transcribe({
  model: elevenlabs.transcription('scribe_v1'),
  audio: new Uint8Array([1, 2, 3, 4]),
  providerOptions: { elevenlabs: { languageCode: 'en' } },
});

The following provider options are available:

    languageCode string

    An ISO-639-1 or ISO-639-3 language code corresponding to the language of the audio file. Can sometimes improve transcription performance if known beforehand. Defaults to null, in which case the language is predicted automatically.

    tagAudioEvents boolean

    Whether to tag audio events like (laughter), (footsteps), etc. in the transcription. Defaults to true.

    numSpeakers integer

    The maximum amount of speakers talking in the uploaded file. Can help with predicting who speaks when. The maximum amount of speakers that can be predicted is 32. Defaults to null, in which case the amount of speakers is set to the maximum value the model supports.

    timestampsGranularity enum

    The granularity of the timestamps in the transcription. Defaults to 'word'. Allowed values: 'none', 'word', 'character'.

    diarize boolean

    Whether to annotate which speaker is currently talking in the uploaded file. Defaults to true.

    fileFormat enum

    The format of input audio. Defaults to 'other'. Allowed values: 'pcm_s16le_16', 'other'. For 'pcm_s16le_16', the input audio must be 16-bit PCM at a 16kHz sample rate, single channel (mono), and little-endian byte order. Latency will be lower than with passing an encoded waveform.
```

## detailed task description & rules

```
Convert text to speech (streaming)

If you prefer to stream the audio directly without saving it to a file, you can use our streaming feature.

import { ElevenLabsClient } from '@elevenlabs/elevenlabs-js';

import * as dotenv from 'dotenv';

dotenv.config();

const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;

if (!ELEVENLABS_API_KEY) {

  throw new Error('Missing ELEVENLABS_API_KEY in environment variables');

}

const elevenlabs = new ElevenLabsClient({

  apiKey: ELEVENLABS_API_KEY,

});

export const createAudioStreamFromText = async (text: string): Promise<Buffer> => {

  const audioStream = await elevenlabs.textToSpeech.stream('JBFqnCBsd6RMkjVDRZzb', {

    modelId: 'eleven_multilingual_v2',

    text,

    outputFormat: 'mp3_44100_128',

    // Optional voice settings that allow you to customize the output

    voiceSettings: {

      stability: 0,

      similarityBoost: 1.0,

      useSpeakerBoost: true,

      speed: 1.0,

    },

  });

  const chunks: Buffer[] = [];

  for await (const chunk of audioStream) {

    chunks.push(chunk);

  }

  const content = Buffer.concat(chunks);

  return content;

};

You can then run this function with:

await createAudioStreamFromText('This is James');
```

the above shows how to do text to speech with elevenlabs in next js

we did it wrongly in app/api/generate-speech/route.ts, fix it with reference to the documentation above. Use the elevenlabs js library instead.

## examples

## [optional] conversation history

## immediate task description or request

NOTE:
- use `bun` not `npm`
- you are banned from running bun commands, eg (bun run dev), let the user do it instead

## [optional] thinking step by step / take a deep breath

## output formatting

## [optional] prefilled response

---

references:
- [Use prompt files in VS Code](https://code.visualstudio.com/docs/copilot/customization/prompt-files)
- [Prompting 101](https://youtube.com/watch?v=ysPbXH0LpIE)
