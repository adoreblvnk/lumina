## task context

I am participating in a global hackathon

**Main Prizes**

*   **1st Place:**
    *   $15k Gemini Credits
    *   $15k OpenAI Credits
    *   $5k JigsawStack Credits
    *   $2.5k Cursor Credits
    *   $1K Groq Credits
    *   $300 Supabase Credits per member
    *   Manus Pro (3 Months)
    *   ElevenLabs Pro (3 Months)

**Side Track Prizes**

*   **ElevenLabs:** Best use of ElevenLabs, an AI voice generator.
    *   Each team member receives 6 months of the Scale tier.
*   **Gemini API:** Top 3 teams using the Gemini API.
    *   $10K credits for each of the Top 3 teams.
*   **Groq:** Best use of Groq, an AI chip company that builds hardware for AI inference.
    *   $1,000 in API Credits, a shoutout on Groq socials, and exclusive swag.
*   **Mem0:** Best Use of Mem0, a universal, self-improving memory layer for LLM applications.
    *   6 months of Mem0 Pro under their Startup Program and a shoutout on Mem0â€™s social channels.

### **Lumina: AI Discussion Facilitator Tech Spec**

**Objective:** A voice-activated AI agent, Lumina, that analyzes small-group classroom discussions in real-time. It provides on-demand, supportive verbal interventions to keep students on topic and engaged, while providing teachers with a high-level dashboard of group performance.

**Tech Stack & Architecture:**
*   **Full-Stack Framework:** Next.js
*   **Language:** TypeScript
*   **UI:** React (via Next.js), Tailwind CSS
*   **Backend Logic:** Next.js API Routes will handle all server-side operations, including communication with external AI services.
*   **AI - Vercel AI SDK:** https://sdk.vercel.ai/
*   **AI - Analysis & Transcription:** Groq (handles real-time transcription, speaker diarization, and analysis of conversational health metrics).
*   **AI - Voice Synthesis:** ElevenLabs
*   **AI - Memory:** (optional) Mem0 to give AI a persistent memory of the conversation.
*   **Hardware:** Single laptop with a microphone per group.

**Environment variables**
- ELEVENLABS_API_KEY
- OPENAI_API_KEY (we will use openai for now)

**Core Features:**

**1. Student Dashboard:**
*   **Voice Onboarding:** Each of the 5 students in a group registers their voice for speaker identification.
*   **Main Interface:** Displays the teacher-assigned discussion prompt (e.g., "Should school students be forced to wear school uniforms"), a countdown timer, and the Lumina chatbot interface.

**2. Teacher Dashboard:**
*   Provides a high-level overview of all groups in the classroom.
*   Displays real-time status and alerts for groups that have received "Severe" intervention warnings, allowing the teacher to provide targeted support.

**Server-Side Logic & AI Integration (via Next.js API Routes):**
*   **Monitoring:** The Next.js backend continuously receives and processes the audio stream, sending it to Groq for analysis against conversational health triggers.
*   **Discussion Mode:** Teachers can tag prompts as "Breadth" or "Depth."
    *   **Breadth:** For topics requiring exploration of multiple viewpoints (e.g., "advantages & disadvantages of universal healthcare").
    *   **Depth:** For topics requiring focused, detailed analysis of a single concept (e.g., "how to write a PEEL paragraph").
*   **Intervention Triggers:**
    1.  **Silence / Low Participation:** Detects extended periods of no speech.
    2.  **Off Topic:** Identifies significant deviation from the core prompt.
    3.  **Imbalanced Discussion (Breadth Mode):** If students neglect key facets of a "Breadth" topic, Lumina prompts them to explore new perspectives.
*   **Two-Tier Intervention Flow:**
    1.  **Mild Intervention:**
        *   **Trigger:** A short period of silence or minor topic drift is detected.
        *   **Action:** The client-side UI displays a non-intrusive "Lumina wants to speak" notification. Students can click a button to hear Lumina's suggestion (e.g., a related question or a gentle nudge back to the topic). If ignored, the request times out.
    2.  **Severe Intervention:**
        *   **Trigger:** A prolonged period of silence or major topic drift is detected.
        *   **Action:** The API route generates a direct, helpful verbal intervention (e.g., "I've noticed we've been quiet for a bit. How about we consider this question...?") and plays it through the speakers via ElevenLabs.
        *   **Teacher Alert:** An alert is simultaneously sent to the Teacher Dashboard.

## [optional] tone context

## background data, documents, & images

file structure:
```
.
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”œâ”€â”€ generate-speech
â”‚   â”‚   â”‚   â””â”€â”€ route.ts
â”‚   â”‚   â”œâ”€â”€ signed-url
â”‚   â”‚   â”‚   â””â”€â”€ route.ts
â”‚   â”‚   â””â”€â”€ transcribe-name
â”‚   â”‚       â””â”€â”€ route.ts
â”‚   â”œâ”€â”€ teacher
â”‚   â”‚   â””â”€â”€ dashboard
â”‚   â”‚       â””â”€â”€ page.tsx
â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”œâ”€â”€ globals.css
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â””â”€â”€ page.tsx
â”œâ”€â”€ components
â”‚   â”œâ”€â”€ ui
â”‚   â”‚   â”œâ”€â”€ button.tsx
â”‚   â”‚   â”œâ”€â”€ card.tsx
â”‚   â”‚   â”œâ”€â”€ input.tsx
â”‚   â”‚   â””â”€â”€ orb.tsx
â”‚   â”œâ”€â”€ background-wave.tsx
â”‚   â”œâ”€â”€ DiscussionView.tsx
â”‚   â”œâ”€â”€ logos.tsx
â”‚   â””â”€â”€ Lumina.tsx
â”œâ”€â”€ lib
â”‚   â””â”€â”€ utils.ts
â”œâ”€â”€ public
â”‚   â””â”€â”€ wave-loop.mp4
â”œâ”€â”€ bun.lock
â”œâ”€â”€ components.json
â”œâ”€â”€ example.ts
â”œâ”€â”€ next.config.ts
â”œâ”€â”€ package.json
â”œâ”€â”€ postcss.config.mjs
â”œâ”€â”€ README.md
â”œâ”€â”€ tailwind.config.ts
â”œâ”€â”€ tsconfig.json
â””â”€â”€ websocket-server.js
```

How to use ai-sdk, an example of using ai-sdk is this:
Agent.ts
```ts
import { anthropic } from "@ai-sdk/anthropic";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";
import { Experimental_Agent as Agent, stepCountIs } from "ai";
import { createCodebaseTools } from "./tools/codebase-tools";
import type { AuthProfile } from "./types";
import { Sandbox } from "e2b";
import { EvalEngine } from "../EvalEngine";

export async function createTestGenAgent(
  sandbox: Sandbox,
  authProfile: AuthProfile,
  sandboxUrl: string | undefined = undefined
) {
  const tools = await createCodebaseTools(sandbox, sandboxUrl, authProfile);

  // Create base agent
  const baseAgent = new Agent({
    model: anthropic("claude-sonnet-4-5"),
    tools: {
      readFile: tools.readFileTool,
      globFiles: tools.globTool,
      searchCodebase: tools.grepTool,
      readTestFile: tools.readTestFile,
      writeTestFile: tools.writeTestFile,
      runTest: tools.runTest,
      checkServerStatus: tools.checkServerStatus,
      browser_navigate: tools.browser_navigate,
      browser_snapshot: tools.browser_snapshot,
      browser_click: tools.browser_click,
      browser_type: tools.browser_type,
      browser_verify_element_visible: tools.browser_verify_element_visible,
    },
    stopWhen: stepCountIs(100), // Increased limit for comprehensive code analysis, testing iteration, and test generation
  });

  // Wrap with evaluation capabilities
  const evalEngine = EvalEngine.create();
  const sheetId = "1kcLNHycMBUpkwlsaLHVeu0IY7t5U2EPtYuYjw05fDUU"; // Same sheet as email agent for now
  return evalEngine.wrap(baseAgent, "testgen-agent", sheetId);
}

```

AgentPrompt.ts
```ts
export function getTestgenAgentPrompt(
  issueTitle: string,
  issueBody: string | null,
  sandboxUrl: string,
  issueNumber: number
): string {
  return "You are a QA Engineer creating Playwright tests. Your ONLY goal: Write tests that verify the feature described in this issue works correctly.\n\n" +

    "ISSUE: " + issueTitle + "\n" +
    "DETAILS: " + (issueBody ?? "") + "\n" +
    "APP URL: " + sandboxUrl + "\n\n" +

    "=== YOUR SINGLE OBJECTIVE ===" +
    "\nðŸŽ¯ Write 1-3 tests that prove this specific feature works (or doesn't work)" +
    "\nðŸŽ¯ Nothing more, nothing less" +
    "\nðŸŽ¯ Don't test the entire app - just answer the issue" +

    // other code here
}

```

TestGenerator.ts
```ts
import { checkIssueIsAlreadyInDB } from "./utils/checkIssueIsAlreadyInDB";
import { SandboxEngine } from "../Sandboxes";
import { createTestGenAgent } from "./Agent";
import { FilestoreInterface } from "../Filestore";
import { getTestgenAgentPrompt } from "./AgentPrompt";
import type { 
  AuthProfile, 
  TestGenerationResult, 
  TestGenerationErrorType,
  TestGenerationSuccessDetails,
  TestGenerationFailureDetails
} from "./types";
import auth from "../../routes/auth";

export class TestGenerator {
  static async generateTest(params: GenerateTestParams): Promise<TestGenerationResult> {
        // other code here
        try {
          agentResult = await testGenAgent.generate({
            prompt: getTestgenAgentPrompt(
              issueTitle,
              issueBody,
              sandboxUrl,
              issueNumber
            ),
          });

// code here
```

here is how to use groq in ai-sdk:
```
You can import the default provider instance groq from @ai-sdk/groq:

import { groq } from '@ai-sdk/groq';

If you need a customized setup, you can import createGroq from @ai-sdk/groq and create a provider instance with your settings:

import { createGroq } from '@ai-sdk/groq';

const groq = createGroq({
  // custom settings
});

You can use the following optional settings to customize the Groq provider instance:

    baseURL string

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is https://api.groq.com/openai/v1.

    apiKey string

    API key that is being sent using the Authorization header. It defaults to the GROQ_API_KEY environment variable.

    headers Record<string,string>

    Custom headers to include in the requests.

    fetch (input: RequestInfo, init?: RequestInit) => Promise<Response>

    Custom fetch

implementation. Defaults to the global fetch function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.
```

here is how to use elevenlabs with aisdk:
```txt
You can create models that call the ElevenLabs transcription API

using the .transcription() factory method.

The first argument is the model id e.g. scribe_v1.

const model = elevenlabs.transcription('scribe_v1');

You can also pass additional provider-specific options using the providerOptions argument. For example, supplying the input language in ISO-639-1 (e.g. en) format can sometimes improve transcription performance if known beforehand.

import { experimental_transcribe as transcribe } from 'ai';
import { elevenlabs } from '@ai-sdk/elevenlabs';

const result = await transcribe({
  model: elevenlabs.transcription('scribe_v1'),
  audio: new Uint8Array([1, 2, 3, 4]),
  providerOptions: { elevenlabs: { languageCode: 'en' } },
});

The following provider options are available:

    languageCode string

    An ISO-639-1 or ISO-639-3 language code corresponding to the language of the audio file. Can sometimes improve transcription performance if known beforehand. Defaults to null, in which case the language is predicted automatically.

    tagAudioEvents boolean

    Whether to tag audio events like (laughter), (footsteps), etc. in the transcription. Defaults to true.

    numSpeakers integer

    The maximum amount of speakers talking in the uploaded file. Can help with predicting who speaks when. The maximum amount of speakers that can be predicted is 32. Defaults to null, in which case the amount of speakers is set to the maximum value the model supports.

    timestampsGranularity enum

    The granularity of the timestamps in the transcription. Defaults to 'word'. Allowed values: 'none', 'word', 'character'.

    diarize boolean

    Whether to annotate which speaker is currently talking in the uploaded file. Defaults to true.

    fileFormat enum

    The format of input audio. Defaults to 'other'. Allowed values: 'pcm_s16le_16', 'other'. For 'pcm_s16le_16', the input audio must be 16-bit PCM at a 16kHz sample rate, single channel (mono), and little-endian byte order. Latency will be lower than with passing an encoded waveform.
```

## detailed task description & rules

we implemented the voice to text websocket wrongly, in `websocket-server.js` and `components/DiscussionView.tsx`

the correct way to handle this is with the official elevenlabs example

```
Next.js implementation example

This example demonstrates how to implement a WebSocket-based conversational agent client in Next.js using the ElevenLabs WebSocket API.

While this example uses the voice-stream package for microphone input handling, you can implement your own solution for capturing and encoding audio. The focus here is on demonstrating the WebSocket connection and event handling with the ElevenLabs API.
1
Install required dependencies

First, install the necessary packages:

npm install voice-stream

The voice-stream package handles microphone access and audio streaming, automatically encoding the audio in base64 format as required by the ElevenLabs API.

This example uses Tailwind CSS for styling. To add Tailwind to your Next.js project:

npm install -D tailwindcss postcss autoprefixer

npx tailwindcss init -p

Then follow the official Tailwind CSS setup guide for Next.js

.

Alternatively, you can replace the className attributes with your own CSS styles.
2
Create WebSocket types

Define the types for WebSocket events:
app/types/websocket.ts

type BaseEvent = {

  type: string;

};

type UserTranscriptEvent = BaseEvent & {

  type: "user_transcript";

  user_transcription_event: {

    user_transcript: string;

  };

};

type AgentResponseEvent = BaseEvent & {

  type: "agent_response";

  agent_response_event: {

    agent_response: string;

  };

};

type AgentResponseCorrectionEvent = BaseEvent & {

  type: "agent_response_correction";

  agent_response_correction_event: {

    original_agent_response: string;

    corrected_agent_response: string;

  };

};

type AudioResponseEvent = BaseEvent & {

  type: "audio";

  audio_event: {

    audio_base_64: string;

    event_id: number;

  };

};

type InterruptionEvent = BaseEvent & {

  type: "interruption";

  interruption_event: {

    reason: string;

  };

};

type PingEvent = BaseEvent & {

  type: "ping";

  ping_event: {

    event_id: number;

    ping_ms?: number;

  };

};

export type ElevenLabsWebSocketEvent =

  | UserTranscriptEvent

  | AgentResponseEvent

  | AgentResponseCorrectionEvent

  | AudioResponseEvent

  | InterruptionEvent

  | PingEvent;

3
Create WebSocket hook

Create a custom hook to manage the WebSocket connection:
app/hooks/useAgentConversation.ts

'use client';

import { useCallback, useEffect, useRef, useState } from 'react';

import { useVoiceStream } from 'voice-stream';

import type { ElevenLabsWebSocketEvent } from '../types/websocket';

const sendMessage = (websocket: WebSocket, request: object) => {

  if (websocket.readyState !== WebSocket.OPEN) {

    return;

  }

  websocket.send(JSON.stringify(request));

};

export const useAgentConversation = () => {

  const websocketRef = useRef<WebSocket>(null);

  const [isConnected, setIsConnected] = useState<boolean>(false);

  const { startStreaming, stopStreaming } = useVoiceStream({

    onAudioChunked: (audioData) => {

      if (!websocketRef.current) return;

      sendMessage(websocketRef.current, {

        user_audio_chunk: audioData,

      });

    },

  });

  const startConversation = useCallback(async () => {

    if (isConnected) return;

    const websocket = new WebSocket("wss://api.elevenlabs.io/v1/convai/conversation");

    websocket.onopen = async () => {

      setIsConnected(true);

      sendMessage(websocket, {

        type: "conversation_initiation_client_data",

      });

      await startStreaming();

    };

    websocket.onmessage = async (event) => {

      const data = JSON.parse(event.data) as ElevenLabsWebSocketEvent;

      // Handle ping events to keep connection alive

      if (data.type === "ping") {

        setTimeout(() => {

          sendMessage(websocket, {

            type: "pong",

            event_id: data.ping_event.event_id,

          });

        }, data.ping_event.ping_ms);

      }

      if (data.type === "user_transcript") {

        const { user_transcription_event } = data;

        console.log("User transcript", user_transcription_event.user_transcript);

      }

      if (data.type === "agent_response") {

        const { agent_response_event } = data;

        console.log("Agent response", agent_response_event.agent_response);

      }

      if (data.type === "agent_response_correction") {

        const { agent_response_correction_event } = data;

        console.log("Agent response correction", agent_response_correction_event.corrected_agent_response);

      }

      if (data.type === "interruption") {

        // Handle interruption

      }

      if (data.type === "audio") {

        const { audio_event } = data;

        // Implement your own audio playback system here

        // Note: You'll need to handle audio queuing to prevent overlapping

        // as the WebSocket sends audio events in chunks

      }

    };

    websocketRef.current = websocket;

    websocket.onclose = async () => {

      websocketRef.current = null;

      setIsConnected(false);

      stopStreaming();

    };

  }, [startStreaming, isConnected, stopStreaming]);

  const stopConversation = useCallback(async () => {

    if (!websocketRef.current) return;

    websocketRef.current.close();

  }, []);

  useEffect(() => {

    return () => {

      if (websocketRef.current) {

        websocketRef.current.close();

      }

    };

  }, []);

  return {

    startConversation,

    stopConversation,

    isConnected,

  };

};

Create the conversation component

Create a component to use the WebSocket hook:
app/components/Conversation.tsx

'use client';

import { useCallback } from 'react';

import { useAgentConversation } from '../hooks/useAgentConversation';

export function Conversation() {

  const { startConversation, stopConversation, isConnected } = useAgentConversation();

  const handleStart = useCallback(async () => {

    try {

      await navigator.mediaDevices.getUserMedia({ audio: true });

      await startConversation();

    } catch (error) {

      console.error('Failed to start conversation:', error);

    }

  }, [startConversation]);

  return (

    <div className="flex flex-col items-center gap-4">

      <div className="flex gap-2">

        <button

          onClick={handleStart}

          disabled={isConnected}

          className="px-4 py-2 bg-blue-500 text-white rounded disabled:bg-gray-300"

        >

          Start Conversation

        </button>

        <button

          onClick={stopConversation}

          disabled={!isConnected}

          className="px-4 py-2 bg-red-500 text-white rounded disabled:bg-gray-300"

        >

          Stop Conversation

        </button>

      </div>

      <div className="flex flex-col items-center">

        <p>Status: {isConnected ? 'Connected' : 'Disconnected'}</p>

      </div>

    </div>

  );

}

Next steps

    Audio Playback: Implement your own audio playback system using Web Audio API or a library. Remember to handle audio queuing to prevent overlapping as the WebSocket sends audio events in chunks.
    Error Handling: Add retry logic and error recovery mechanisms
    UI Feedback: Add visual indicators for voice activity and connection status

Latency management

To ensure smooth conversations, implement these strategies:

    Adaptive Buffering: Adjust audio buffering based on network conditions.
    Jitter Buffer: Implement a jitter buffer to smooth out variations in packet arrival times.
    Ping-Pong Monitoring: Use ping and pong events to measure round-trip time and adjust accordingly.
```

## examples

## [optional] conversation history

## immediate task description or request

NOTE:
- use `bun` not `npm`
- you are banned from running bun commands, eg (bun run dev), let the user do it instead

## [optional] thinking step by step / take a deep breath

## output formatting

## [optional] prefilled response

---

references:
- [Use prompt files in VS Code](https://code.visualstudio.com/docs/copilot/customization/prompt-files)
- [Prompting 101](https://youtube.com/watch?v=ysPbXH0LpIE)
